detailed description of your own approach, discussion of alternatives, introduction of the experimental methodology and quality metrics


We started with the initial assumption that the project can be divided into two challenges: Firstly, the classification, and secondly the localization. We found an approach that allows for both challenges to be addressed together relatively early on. Nonetheless we will firstly cover an early alternative approach, because it will help us showcase some inherent difficulties of the challenges that had to be overcome.

We firstly decided to approach this as a problem where the images are, in a certain way, of a too high resolution. We mean this in the sense that there are tools and tissue, with the tissue obscuring the tools. We therefore followed the approach of Sun et.al (2021) to use a variational auto-encoder to reduce the images to little more than the tools, since those should be considered the most important parts of the image, or are at least the parts easiest to recognize for the De- and Encoder, since the tools appear more often than the respective tissue-structures.

Image.

Han et.al. (2017) presented a relatively straightforward VAE-model, that we decided to rebuild and adapt to our needs. 

Image (Model and Result by Huan)

Since we don't need any representation that keeps too much of the original image, we mainly trimmed the whole approach down.
In the end, so we hoped. the instruments would be the parts that would at least to some extend be "saved over" to the other side of the image. We also experimented with the code (z), hoping it might encode the information preserving locality, so that the interpretation of the code gives clues already. For this purpose we settled at around 500 latent variables, since less would not be useful in terms of placing an object within an image. 

The more straightforward approach here is of course threading the image through a needle, meaning a rather small latent variable space again, and then analysing the reconstruction. The hope was, that the image would preserve well distinguishable blobs where the tools were, so that we could filter the localization of the tools with some old-fashioned blob-detection via Otsus method or similar approaches (https://en.wikipedia.org/wiki/Otsu%27s_method).

However, although the concept of VAEs is very intriguing, there is still an issue to address: Given a successful localization of objects, how does one classify the image?
The challenge is, that even when given only the part of the image with the tool itself, this would still leave us with having to solve a rather unpleasant classification problem, since all the images come annotated with all the classes originally in the image. This problem accompanied us during the whole project. The advantage in this case is, that we would stick with the well-known one-label-classification problem, meaning that ideally only one label is true at any point, although the calculation of the loss would be difficult. We are confident that this problem would have been solvable as well. But searching for a solution for the image-classification-part, we found that there was already a better approach.

We build a first prototype of the VAE, but training was impossible at first due to the handling of the huge dataset that we used first, that comprised of over 100GB of image data. More on that in the experimentation-section. However, the sheer amount of data was not the main issue, but it points to another issue: We could not simply take the data, split it up into a training-, validation- and test-set and perform SGD on the training-set. 
This has two reasons: Firstly, the data was very unbalanced (see table 1). Secondly, and more importantly, we had to somehow make sure that the model would not learn something else, an issue that can be illustrated by some funny and some rather sinister examples (https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/).

Table: Balance of data.

To this end swapped the colour-channels randomly, flipped the image with a 50\% chance, rotated it between 0 and 90 degrees, masked and cropped the images. Masking means that we set some regions of the image to the mean value of the rest of the image. The regions where selected in the following way: Any image was divided into 24x24-tiles, with every tile having a 50\% chance of being masked. Since this is basically the binomial distribution we can expect that around half of the tiles are masked everytime. This can look like this:

masked image:

Another measure covered in the lecture on data-augmentation was the fact, that some falsely-annotated data put into the training-set might also serve the generalization-abilities of the network. We decided against any such measure, since the multi-classification of the entire clip presented us with more falsely annotated images than we might have wished for.

Having the data handled we went on exploring possible solutions to classifying the images. We therefore turned to CNNs, that, as we covered in the lecture, proved quite useful in image-classification-challenges. One interesting aspect of CNNs like the ResNET-models is that, before reducing the output of the last hidden layer to logits of the class-probabilities, these models preserve some of the size and locality of the input since the transformations are mainly convolutional in character. This means that the ResNets forward-pass can be intercepted, while the locality of the discovered features is preserved.
This also means that by computing a fully connected convolution one gets a heat-map for all the tools. We learnt about this procedure from the paper by ..., but it is fairly common already. It is quite obvious that a well-performing network that returns heat-maps of the tools presence renders any other effort to localize tools obsolete. Since this is the concept we eventually pursued, we will cover it in more detail:

The final model:
For the underlying architecture of the final model we followed a straightforward and established approach: We worked with a well-performing backbone to extract features of the image and trained a fully connected convolutional layer based off of those features. This results in a heat-map, that can be further processed.
		The intuition: Perhaps an image that shows how that is meant.
The fully connected convolutional layer replaces the sliding-window-approach. 

