Reliable surgical tool localization is a milestone on the way to applications that enable assessments of surgical performance and efficiency, identifications of the skillful use of instruments and choreographies as well as planning of operative and logistical aspects of surgical resources. In addition, the ability to automatically detect and track surgical instruments in endoscopic videos paves the way for much more complex problems such as computer-assisted or even fully automated surgery. This could foster advances in the field of telemedicine, for example, by taking on simpler parts of surgeries fully automatically, or by making the automatic delivery of instruments more fluid. These advances are only possible with the help of large data sets indicating not only the presence but also the location of instruments. This is a difficult task because it is tedious and time consuming to obtain the annotations needed to train machine learning models. This is because the bounding boxes in videos have to be labelled frame by frame so far, and for a large number of surgical tools and operations. In addition, annotators need to be trained to keep up with innovations in surgical instruments. Automated localization and classification of surgical tools makes it possible to create higher-quality datasets cost-effectively by allowing a model to learn to classify and localize instruments based on a weakly annotated dataset. Weakly annotated in this context means that for each clip only the presence of the respective tools is given.